{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Variational Autoencoder\n",
    "> In this post, the variational autoencoder (VAE) is used for anomaly detection on the UC Berkeley milling data set. A variational autoencoder is more expressive than a regular autoencoder, and this feature can be exploited for anomaly detection. (notebook originally featured at [tvhahn.com](https://www.tvhahn.com/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve explored the UC Berkeley milling data set – now it’s time for us to build some models! In part iV of this series, we discussed how an autoencoder can be used for anomaly detection. However, in this part, we’ll use a variant of the autoencoder – a variational autoencoder (VAE) – to conduct the anomaly detection. You’ll soon see that the VAE is similar, and different, from the traditional autoencoder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook can be run with google colab. Alternatively, clone the repo and run on your local machine. You'll need python 3.6+ with the following packages in your local environment:\n",
    "\n",
    "* Numpy\n",
    "* SciPy\n",
    "* Pandas\n",
    "* Matplotlib\n",
    "* Seaborn\n",
    "\n",
    "First, we will load all the neccessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio # for reading matlab files\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the appropriate working folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move into the root directory of 'Manufacturing-Data-Science-with-Python'\n",
    "os.chdir('../')\n",
    "root_dir = Path.cwd() # set the root directory as a Pathlib path\n",
    "\n",
    "folder_raw_data = root_dir / 'Data Sets/milling_uc_berkeley/raw/' # raw data folder that holds the .zip .mat files for milling data\n",
    "folder_processed_data = root_dir / 'Data Sets/milling_uc_berkeley/processed/' # processed data folder\n",
    "working_folder = root_dir / 'Metal Machining' # working folder\n",
    "\n",
    "folder_models = folder_processed_data / 'models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to prepare the notebook by downloading the milling data file and other important files. This needs to be done if you are running in google colab. If the repository has been cloned from github, then there is no need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the the raw data folder does not exist, then you are likely\n",
    "# in a google colab environment. In that case, we will create the \n",
    "# raw data and processed data folders and download the appropriate\n",
    "# files\n",
    "if folder_raw_data.exists() == False:\n",
    "\n",
    "    pathlib.Path(folder_raw_data).mkdir(parents=True, exist_ok=True)\n",
    "    os.chdir(folder_raw_data)\n",
    "    !wget 'https://github.com/tvhahn/Manufacturing-Data-Science-with-Python/raw/master/Data%20Sets/milling_uc_berkeley/raw/mill.zip'\n",
    "    \n",
    "    \n",
    "if folder_processed_data.exists() == False:\n",
    "\n",
    "    pathlib.Path(folder_processed_data).mkdir(parents=True, exist_ok=True)\n",
    "    os.chdir(folder_processed_data)\n",
    "    !wget 'https://raw.githubusercontent.com/tvhahn/Manufacturing-Data-Science-with-Python/master/Data%20Sets/milling_uc_berkeley/processed/labels_with_tool_class.csv'\n",
    "\n",
    "# if working folder does not exist, then create it\n",
    "pathlib.Path(working_folder).mkdir(parents=True, exist_ok=True)\n",
    "if (working_folder / 'data_prep.py').exists() == False:\n",
    "    os.chdir(working_folder)   \n",
    "    # download important python files into the 'Metal Machining' directory\n",
    "    !wget 'https://raw.githubusercontent.com/tvhahn/Manufacturing-Data-Science-with-Python/master/Metal%20Machining/data_prep.py'\n",
    "    !wget 'https://raw.githubusercontent.com/tvhahn/Manufacturing-Data-Science-with-Python/master/Metal%20Machining/tcn.py'\n",
    "\n",
    "    \n",
    "if folder_models.exists() == False:\n",
    "\n",
    "    pathlib.Path(folder_models / 'best_models').mkdir(parents=True, exist_ok=True)\n",
    "    pathlib.Path(folder_models / 'saved_models').mkdir(parents=True, exist_ok=True)\n",
    "    os.chdir(folder_models / 'best_models')\n",
    "    !wget 'https://github.com/tvhahn/ml-tool-wear/raw/master/models/best_models/best_models.zip'\n",
    "    \n",
    "    \n",
    "# extract mill.mat from the zip file\n",
    "with zipfile.ZipFile(folder_raw_data / 'mill.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall(folder_raw_data)\n",
    "    \n",
    "os.chdir(working_folder)\n",
    "\n",
    "# import those packages that we just downloaded\n",
    "import data_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Variational Autoencoder \n",
    "\n",
    "The variational autoencoder was introduced in 2013 and today is widely used in machine learning applications. The VAE is different from traditional autoencoders in that the VAE is both probabilistic and generative. What does that mean? Well, the VAE creates outputs that are partly random (even after training) and can also generate new data that is like the data it is trained on. \n",
    "\n",
    "Again, there are excellent explanations of the VAE online – I'll direct you to Alfredo Canziani’s deep learning course (from [his Youtube](https://www.youtube.com/embed/7Rb4s9wNOmc)). But here is my attempt at an explanation: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a high level, the VAE has a similar structure to a traditional autoencoder. However, the encoder learns different codings; namely, the VAE learns mean codings, $\\boldsymbol{\\mu}$, and standard deviation codings, $\\boldsymbol{\\sigma}$. The VAE then samples randomly from a Gaussian distribution, with the same mean and standard deviation created by the encoder, to generate the latent variables, $\\boldsymbol{z}$. These latent variables are then “decoded” to reconstruct the input. The figure below demonstrates how a signal is reconstructed using the VAE. \n",
    "\n",
    "<div style=\"text-align: center; \">\n",
    "<figure>\n",
    "  <img src=\"images/vae.svg\" alt=\"variational autoencoder process\" style=\"background:none; border:none; box-shadow:none; text-align:center\" width=\"800px\"/>\n",
    "  <div style=\"text-align: left; \">\n",
    "  <figcaption style=\"color:grey; font-size:smaller\">A variational autoencoder architecture (top), and an example of a data sample going through the VAE (bottom). Data is compressed in the encoder to create mean and standard deviation codings. The coding, $\\boldsymbol{z}$, is then created, with the addition of Gaussian noise, from the mean and standard deviation codings. The decoder uses the codings (or latent variables) $\\boldsymbol{z}$ to reconstruct the input. (Image from author, based on graphic from Aurélien Geron)</figcaption>\n",
    "  </div>\n",
    "</figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, the VAE works to minimize its reconstruction loss, and at the same time, force a Gaussian structure using a latent loss. The structure is achieved through the Kullback-Leibler (KL) divergence, with detailed derivations for the losses in the original VAE paper.[^kingma2013auto] Together, the reconstruction loss, and latent loss are as follows: \n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "$$\\mathcal{L}(\\boldsymbol{x},\\boldsymbol{x'}) = \\left \\| \\boldsymbol{x}-\\boldsymbol{x'} \\right \\|^{2} + \\frac{\\beta}{2}\\sum_{i=1}^{K}( \\sigma_i - \\log(\\sigma_i) -1 +  \\mu_i^2)$$\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "where $K$ is the number of latent variables, and $\\beta$ is an adjustable hyper-parameter introduced by Higgens et al.[^higgins2016beta] \n",
    "\n",
    "A VAE learns factors, embedded in the codings, that can be used to generate new data. As an example of these factors, a VAE may be trained to recognize shapes in an image. One factor may encode information on how pointy the shape is, while another factor may look at how round it is. However, in a VAE, the factors are often entangled together across the codings (the latent variables). Tuning the hyper-parameter $\\beta$, to a value larger than one, can enable the factors to disentangle such that each coding only represents one factor at a time. Thus, greater interpretability of the model can be obtained. A VAE with a tunable beta is often called a disentangled-variational-autoencoder, or simply, a $\\beta$-VAE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "Before going any further, we need to prepare the data. Ultimately, we'll be using the VAE to detect \"abnormal\" tool conditions, which correspond to when the tool is in a worn. But first we need to label the data.\n",
    "\n",
    "As shown in the last post, each cut has an associated amount of wear, measured at the end of the cut. We'll label each cut as either healthy, degraded, or failed according to the amount of wear on the tool. Here's the schema:\n",
    "\n",
    "| State    | Label | Flank Wear (mm) |\n",
    "| -------- | ----: | --------------: |\n",
    "| Healthy  |     0 |         0 ~ 0.2 |\n",
    "| Degraded |     1 |       0.2 ~ 0.7 |\n",
    "| Failed   |     2 |           > 0.7 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've created a data prep class that takes the raw matlab files, a labelled CSV (each cut is labelled with the associated flank wear), and spits out the training/validation/and testing data (you can see the entire data_prep.py in the github repository). However, I want to highlight one function in the class that is important; that is, the create_tensor function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensor(self, data_sample, signal_names, start, end, window_size, stride=8):\n",
    "    \"\"\"Create a tensor from a cut sample. Final tensor will have shape: \n",
    "    [# samples, # sample len, # features/sample]\n",
    "\n",
    "    Parameters\n",
    "    ===========\n",
    "    data_sample : ndarray\n",
    "        single data sample (individual cut) containing all the signals\n",
    "\n",
    "    signal_names : tuple\n",
    "        tuple of all the signals that will be added into the tensor\n",
    "\n",
    "    start : int\n",
    "        starting index (e.g. the first 2000 \"samples\" in the cut may be from\n",
    "        when the tool is not up to speed, so should be ignored)\n",
    "\n",
    "    end : int\n",
    "        ending index\n",
    "\n",
    "    window_size : int\n",
    "        size of the window to be used to make the sub-cuts\n",
    "\n",
    "    stride : int\n",
    "        length to move the window at each iteration\n",
    "\n",
    "    Returns\n",
    "    ===========\n",
    "    c : ndarray\n",
    "        array of the sub-cuts\n",
    "    \"\"\"\n",
    "\n",
    "    s = signal_names[::-1]  # only include the six signals, and reverse order\n",
    "    c = data_sample[s[0]].reshape((9000, 1))\n",
    "\n",
    "    for i in range(len(s)):\n",
    "        try:\n",
    "            a = data_sample[s[i + 1]].reshape((9000, 1))  # reshape to make sure\n",
    "            c = np.hstack((a, c))  # horizontal stack\n",
    "        except:\n",
    "            # reshape into [# samples, # sample len, # features/sample]\n",
    "            c = c[start:end]\n",
    "            c = np.reshape(c, (c.shape[0], -1))\n",
    "\n",
    "    dummy_array = []\n",
    "    # fit the strided windows into the dummy_array until the length\n",
    "    # of the window does not equal the proper length\n",
    "    for i in range(c.shape[0]):\n",
    "        windowed_signal = c[i * stride : i * stride + window_size]\n",
    "        if windowed_signal.shape == (window_size, 6):\n",
    "            dummy_array.append(windowed_signal)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    c = np.array(dummy_array)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The create_tensor function takes an individual cut, breaks it up into chunks, and puts them into a single array. It breaks the cut signal up into chunks using a window of a fixed size (the window_size variable) and then \"slides\" the window along the signal. The window \"slides\" by a preditermined amount, set by the stride variable.\n",
    "\n",
    "We'll take each of the 165 cuts (remember, two cuts from the original 167 are corrupted) and apply a window size of 64 and a stride of 64 (no overlap between windows). I've visually inspected each cut and selected when the \"stable\" cutting region occurs, which is usually five seconds or so after the signal begins collection, and a few seconds before the signal collection ends. This information is stored in the \"labels_with_tool_class.csv\" file.\n",
    "\n",
    "With all that, we can then create the training/validation/testing data sets. Here is what the script looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of X_train: (7751, 64, 6)\n",
      "Shape of y_train: (7751,)\n",
      "\n",
      "Shape of X_val: (1909, 64, 6)\n",
      "Shape of y_val: (1909,)\n",
      "\n",
      "Shape of X_test: (1910, 64, 6)\n",
      "Shape of y_test: (1910,)\n",
      "\n",
      "Shape of X_train_slim: (2831, 64, 6)\n",
      "Shape of y_train_slim: (2831,)\n",
      "\n",
      "Shape of X_val_slim: (697, 64, 6)\n",
      "Shape of y_val_slim: (697,)\n",
      "shape y_train: (7751,) \t\t0: 36.5% \t\t1: 56.2% \t\t2: 7.3%\n",
      "shape y_val: (1909,) \t\t0: 36.5% \t\t1: 56.2% \t\t2: 7.3%\n",
      "shape y_test: (1910,) \t\t0: 36.5% \t\t1: 56.2% \t\t2: 7.3%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import data_prep\n",
    "\n",
    "data_file = folder_raw_data / \"mill.mat\"\n",
    "\n",
    "\n",
    "# helper functions needed in the data processing\n",
    "def scaler(x, min_val_array, max_val_array):\n",
    "    '''Scale an array across all dimensions'''\n",
    "    \n",
    "    # get the shape of the array\n",
    "    s, _, sub_s = np.shape(x)\n",
    "    \n",
    "    for i in range(s):\n",
    "        for j in range(sub_s):\n",
    "            x[i,:,j] = np.divide((x[i,:,j] - min_val_array[j]), np.abs(max_val_array[j] - min_val_array[j]))\n",
    "           \n",
    "    return x\n",
    "\n",
    "# min-max function\n",
    "def get_min_max(x):\n",
    "    '''Get the minimum and maximum values for each\n",
    "        dimension in an array'''\n",
    "    \n",
    "    # flatten the input array http://bit.ly/2MQuXZd\n",
    "    flat_vector = np.concatenate(x)\n",
    "\n",
    "    min_vals = np.min(flat_vector,axis=0)\n",
    "    max_vals = np.max(flat_vector,axis=0)\n",
    "\n",
    "    return min_vals, max_vals\n",
    "    \n",
    "# use the DataPrep module to load the data\n",
    "prep = data_prep.DataPrep(data_file)\n",
    "\n",
    "# load the labeled CSV (NaNs filled in by hand)\n",
    "df_labels = pd.read_csv(folder_processed_data / \"labels_with_tool_class.csv\")\n",
    "\n",
    "\n",
    "# Save regular data set. The X_train, X_val, X_test will be used for anomaly detection\n",
    "# discard certain cuts as they are strange\n",
    "cuts_remove = [17, 94]\n",
    "df_labels.drop(cuts_remove, inplace=True)\n",
    "\n",
    "# use the return_xy function to take the milling data, select the stable cutting region,\n",
    "# and apply the window/stride\n",
    "X, y = prep.return_xy(df_labels, prep.data,prep.field_names[7:],window_size=64, stride=64)\n",
    "\n",
    "# use sklearn train_test_split function\n",
    "# use stratify to ensure that the distribution of classes is equal\n",
    "# between each of the data sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=15, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.50, random_state=10, stratify=y_test)\n",
    "\n",
    "# get the min/max values from each dim in the X_train\n",
    "min_vals, max_vals = get_min_max(X_train)\n",
    "\n",
    "# scale the train/val/test with the above min/max values\n",
    "X_train = scaler(X_train, min_vals, max_vals)\n",
    "X_val = scaler(X_val, min_vals, max_vals)\n",
    "X_test = scaler(X_test, min_vals, max_vals)\n",
    "\n",
    "# training/validation of VAE is only done on healthy (class 0)\n",
    "# data samples, so we must remove classes 1, 2 from the train/val\n",
    "# data sets\n",
    "X_train_slim, y_train_slim = prep.remove_classes(\n",
    "    [1,2], y_train, X_train\n",
    ")\n",
    "\n",
    "\n",
    "X_val_slim, y_val_slim = prep.remove_classes(\n",
    "    [1,2], y_val, X_val\n",
    ")\n",
    "\n",
    "print(\"\\nShape of X_train:\", X_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"\\nShape of X_val:\", X_val.shape)\n",
    "print(\"Shape of y_val:\", y_val.shape)\n",
    "print(\"\\nShape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)\n",
    "print(\"\\nShape of X_train_slim:\", X_train_slim.shape)\n",
    "print(\"Shape of y_train_slim:\", y_train_slim.shape)\n",
    "print(\"\\nShape of X_val_slim:\", X_val_slim.shape)\n",
    "print(\"Shape of y_val_slim:\", y_val_slim.shape)\n",
    "\n",
    "# function to show the percentage of labels in y-labels\n",
    "def y_shape_percentage(y_train, label):\n",
    "    \n",
    "    l = y_train\n",
    "    \n",
    "    print('shape {}:'.format(label), l.shape, \n",
    "          '\\t\\t0: {:.1%}'.format(len(l[l == 0])/len(l)), \n",
    "          '\\t\\t1: {:.1%}'.format(len(l[l == 1])/len(l)), \n",
    "          '\\t\\t2: {:.1%}'.format(len(l[l == 2])/len(l)))\n",
    "    \n",
    "# let's see what percentage of the data set splits are made up of healthy (0), degraded (1),\n",
    "# and failed (2) labels\n",
    "y_shape_percentage(y_train, 'y_train')\n",
    "y_shape_percentage(y_val, 'y_val')\n",
    "y_shape_percentage(y_test, 'y_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final distribution of the data is shown below. As noted in [Part IV](https://www.tvhahn.com/posts/anomaly-detection-with-autoencoders/) of this series, we'll be training the VAE on only the healthy data (class 0). However, checking the perfomance anomaly detection will be done using all the data. In other words, we'll be training our VAE on the \"slim\" data sets.\n",
    "\n",
    "| Data Split      | No. Sub-Cuts | Healthy % | Degraded % | Failed % |\n",
    "| --------------- | :----------: | :-------: | :--------: | :------: |\n",
    "| Training full   |    11,570    |   36.5    |    56.2    |   7.3    |\n",
    "| Training slim   |    2,831     |    100    |     -      |    -     |\n",
    "| Validation full |    1,909     |   36.5    |    56.2    |   7.3    |\n",
    "| Validation slim |     697      |    100    |     -      |    -     |\n",
    "| Testing full    |    1,910     |   36.5    |    56.2    |   7.3    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Model\n",
    "\n",
    "We now understand what a variational autoencoder is and how the data is prepared. Time to build! \n",
    "\n",
    "Our VAE will be made up of layers consisting of convolutions layers, batch normalization layers, and max pooling layers. The figure below shows what one of our VAE models could look like. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; \">\n",
    "<figure>\n",
    "  <img src=\"images/model_architecture.svg\" alt=\"variational autoencoder process\" style=\"background:none; border:none; box-shadow:none; text-align:center\"/>\n",
    "  <div style=\"text-align: left; \">\n",
    "  <figcaption style=\"color:grey; font-size:smaller\">Example model architecture used in the $\\beta$-VAE. The input to the encoder is a milling data sample, with a window size of 64 for an input shape of [64, 6]. There are 3 convolutional layers, a filter size of 17, and a coding size of 18. (Image from author)</figcaption>\n",
    "  </div>\n",
    "</figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I won't be going through all the details of the model -- you can look at the Jupyter notebook and read my code comments. However, here are some important points:\n",
    "\n",
    "* I've used the temporal convolutional network as the basis for the convolutional layers. The implementation is from Philippe Remy -- thanks Philippe! You can find his github repo [here](https://github.com/philipperemy/keras-tcn).\n",
    "* Aurélien Geron's book, [\"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\"](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646/ref=sr_1_1?dchild=1&keywords=Aur%C3%A9lien+Geron&qid=1614346360&sr=8-1), is great. In particular, his section on VAEs was incredibly helpful, and I've used some of his methods here. There is a Jupyter notebook from that section of his book on [his github](https://github.com/ageron/handson-ml2/blob/master/17_autoencoders_and_gans.ipynb). Thanks Aurélien! [^geron2019hands]\n",
    "* I've used rounded accuracy to measure how the model performs during training, as suggested by Geron.\n",
    "\n",
    "Here is, roughly, what the model function looks like. Note, I've also included the Sampling class and rounded accuracy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.1.0\n",
      "Keras version:  2.2.4-tf\n",
      "Tensorboard version: 2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorboard\n",
    "from tcn import TCN\n",
    "import numpy as np\n",
    "import datetime\n",
    "import traceback\n",
    "\n",
    "# see what version of tensorflow is installed\n",
    "print('TensorFlow version: ', tf.__version__)\n",
    "print('Keras version: ', keras.__version__)\n",
    "print('Tensorboard version:', tensorboard.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device not found\n"
     ]
    }
   ],
   "source": [
    "# see if you're using a GPU. You don't have to, but it\n",
    "# will be faster if you're training many models\n",
    "try:\n",
    "    device_name = tf.test.gpu_device_name()\n",
    "    if device_name != '/device:GPU:0':\n",
    "        raise SystemError('GPU device not found')\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "except:\n",
    "    print('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build custom sampling function\n",
    "# Sampling and rounded_accuracy code modified from Aurelion Geron, \n",
    "# https://github.com/ageron/handson-ml2/blob/master/17_autoencoders_and_gans.ipynb\n",
    "# used under Apache 2.0 License, https://github.com/ageron/handson-ml2/blob/master/LICENSE\n",
    "\n",
    "K = keras.backend\n",
    "\n",
    "# class for sampling embeddings in the latent space\n",
    "class Sampling(keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        mean, log_var = inputs\n",
    "        return K.random_normal(tf.shape(log_var)) * K.exp(log_var / 2) + mean\n",
    "\n",
    "\n",
    "# rounded accuracy for the metric\n",
    "def rounded_accuracy(y_true, y_pred):\n",
    "    return keras.metrics.binary_accuracy(tf.round(y_true), tf.round(y_pred))\n",
    "\n",
    "\n",
    "# fit the model\n",
    "def model_fit(\n",
    "    X_train_slim,\n",
    "    X_val_slim,\n",
    "    beta_value=1.25,\n",
    "    codings_size=10,\n",
    "    dilations=[1, 2, 4],\n",
    "    conv_layers=1,\n",
    "    seed=31,\n",
    "    start_filter_no=32,\n",
    "    kernel_size_1=2,\n",
    "    epochs=10,\n",
    "    earlystop_patience=8,\n",
    "    verbose=0,\n",
    "    compile_model_only=False,\n",
    "):\n",
    "\n",
    "    _, window_size, feat = X_train_slim.shape\n",
    "\n",
    "\n",
    "    # save the time model training began\n",
    "    # this way we can identify trained model at the end\n",
    "    date_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    \n",
    "    # set random seeds so we can somewhat reproduce results\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    end_filter_no = start_filter_no\n",
    "\n",
    "    inputs = keras.layers.Input(shape=[window_size, feat])\n",
    "    z = inputs\n",
    "\n",
    "    #### ENCODER ####\n",
    "    for i in range(0, conv_layers):\n",
    "        z = TCN(\n",
    "            nb_filters=start_filter_no,\n",
    "            kernel_size=kernel_size_1,\n",
    "            nb_stacks=1,\n",
    "            dilations=dilations,\n",
    "            padding=\"causal\",\n",
    "            use_skip_connections=True,\n",
    "            dropout_rate=0.0,\n",
    "            return_sequences=True,\n",
    "            activation=\"selu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            use_batch_norm=False,\n",
    "            use_layer_norm=False,\n",
    "        )(z)\n",
    "\n",
    "        z = keras.layers.BatchNormalization()(z)\n",
    "        z = keras.layers.MaxPool1D(pool_size=2)(z)\n",
    "\n",
    "    z = keras.layers.Flatten()(z)\n",
    "    print(\"Shape of Z:\", z.shape)\n",
    "\n",
    "    codings_mean = keras.layers.Dense(codings_size)(z)\n",
    "\n",
    "    codings_log_var = keras.layers.Dense(codings_size)(z)\n",
    "\n",
    "    codings = Sampling()([codings_mean, codings_log_var])\n",
    "\n",
    "    variational_encoder = keras.models.Model(\n",
    "        inputs=[inputs], outputs=[codings_mean, codings_log_var, codings]\n",
    "    )\n",
    "\n",
    "    #### DECODER ####\n",
    "    decoder_inputs = keras.layers.Input(shape=[codings_size])\n",
    "\n",
    "    x = keras.layers.Dense(\n",
    "        start_filter_no * int((window_size / (2 ** conv_layers))), activation=\"selu\"\n",
    "    )(decoder_inputs)\n",
    "\n",
    "    x = keras.layers.Reshape(\n",
    "        target_shape=((int(window_size / (2 ** conv_layers))), end_filter_no)\n",
    "    )(x)\n",
    "\n",
    "    for i in range(0, conv_layers):\n",
    "        x = keras.layers.UpSampling1D(size=2)(x)\n",
    "        x = keras.layers.BatchNormalization()(x)\n",
    "\n",
    "        x = TCN(\n",
    "            nb_filters=start_filter_no,\n",
    "            kernel_size=kernel_size_1,\n",
    "            nb_stacks=1,\n",
    "            dilations=dilations,\n",
    "            padding=\"causal\",\n",
    "            use_skip_connections=True,\n",
    "            dropout_rate=0.0,\n",
    "            return_sequences=True,\n",
    "            activation=\"selu\",\n",
    "            kernel_initializer=\"he_normal\",\n",
    "            use_batch_norm=False,\n",
    "            use_layer_norm=False,\n",
    "        )(x)\n",
    "\n",
    "    outputs = keras.layers.Conv1D(\n",
    "        feat, kernel_size=kernel_size_1, padding=\"same\", activation=\"sigmoid\"\n",
    "    )(x)\n",
    "    variational_decoder = keras.models.Model(inputs=[decoder_inputs], outputs=[outputs])\n",
    "\n",
    "    _, _, codings = variational_encoder(inputs)\n",
    "    reconstructions = variational_decoder(codings)\n",
    "    variational_ae_beta = keras.models.Model(inputs=[inputs], outputs=[reconstructions])\n",
    "\n",
    "    latent_loss = (\n",
    "        -0.5\n",
    "        * beta_value\n",
    "        * K.sum(\n",
    "            1 + codings_log_var - K.exp(codings_log_var) - K.square(codings_mean),\n",
    "            axis=-1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    variational_ae_beta.add_loss(K.mean(latent_loss) / (window_size * feat))\n",
    "    variational_ae_beta.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=\"adam\",  #'rmsprop'\n",
    "        metrics=[rounded_accuracy],\n",
    "    )\n",
    "\n",
    "    # count the number of parameters so we can look at this later\n",
    "    # when evaluating models\n",
    "    param_size = \"{:0.2e}\".format(\n",
    "        variational_encoder.count_params() + variational_decoder.count_params()\n",
    "    )\n",
    "\n",
    "\n",
    "    # Model Name\n",
    "    # b : beta value used in model\n",
    "    # c : number of codings -- latent variables\n",
    "    # l : numer of convolutional layers in encoder (also decoder)\n",
    "    # f1 : the starting number of filters in the first convolution\n",
    "    # k1 : kernel size for the first convolution\n",
    "    # k2 : kernel size for the second convolution\n",
    "    # d : whether dropout is used when sampling the latent space (either True or False)\n",
    "    # p : number of parameters in the model (encoder + decoder params)\n",
    "    # eps : number of epochs\n",
    "    # pat : patience stopping number\n",
    "\n",
    "    model_name = (\n",
    "        \"TBVAE-{}:_b={:.2f}_c={}_l={}_f1={}_k1={}_dil={}\"\n",
    "        \"_p={}_eps={}_pat={}\".format(\n",
    "            date_time,\n",
    "            beta_value,\n",
    "            codings_size,\n",
    "            conv_layers,\n",
    "            start_filter_no,\n",
    "            kernel_size_1,\n",
    "            dilations,\n",
    "            param_size,\n",
    "            epochs,\n",
    "            earlystop_patience,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\"\\n\", model_name, \"\\n\")\n",
    "\n",
    "    if compile_model_only == False:\n",
    "\n",
    "        earlystop_callback = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            min_delta=0.0002,\n",
    "            patience=earlystop_patience,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1,\n",
    "        )\n",
    "\n",
    "        history = variational_ae_beta.fit(\n",
    "            X_train_slim,\n",
    "            X_train_slim,\n",
    "            epochs=epochs,\n",
    "            batch_size=1024,\n",
    "            shuffle=True,\n",
    "            validation_data=(X_val_slim, X_val_slim),\n",
    "            callbacks=[earlystop_callback,],  # tensorboard_callback,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        return date_time, model_name, history, variational_ae_beta, variational_encoder\n",
    "\n",
    "    else:\n",
    "\n",
    "        return variational_ae_beta, variational_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "Time to begin training some models. To select the hyperparameters, we'll be using a random search. Why a random search? Well, it's fairly simple to implement and has been shown to yield better results when compared to a grid search.[^bergstra2012random] Scikit-learn has some nice methods for implementing a random search, and that's what we'll use.\n",
    "\n",
    "We'll be training a bunch of different VAEs, all with different parameters. After each VAE has been trained (trained to minimize reconstruction loss) and the model saved, we'll go through the VAE model and see how it performs in anomaly detection. Here's a diagram of the random search training process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; \">\n",
    "<figure>\n",
    "  <img src=\"images/vae_training_random_search.png\" alt=\"random search training process\" style=\"background:none; border:none; box-shadow:none; text-align:center\"/>\n",
    "  <div style=\"text-align: left; \">\n",
    "  <figcaption style=\"color:grey; font-size:smaller\">The random search training process has three steps. First, randomly select the hyperparameters. Second, train the VAE with these parameters. Third, check the anomaly detection performance of the trained VAE.  (Image from author)</figcaption>\n",
    "  </div>\n",
    "</figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, when I ran this experiment, I trained about 1000 VAE models on Google Colab (yay free GPUs!). After all 1000 models were trained, I moved them to my local computer, with a less powerful GPU, and then checked the models for anomaly detection performance. Google Colab has access to great GPUs, but your time with them is limited, so it makes sense to maximize the use of the GPUs on them this way. I'll detail how the trained models are checked for anomaly detection performance in the next blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input the number of iterations you want to search over\n",
    "random_search_iterations = 1\n",
    "\n",
    "# random seed value from system input\n",
    "ransdom_seed_input = 135\n",
    "\n",
    "# parameters for beta-vae\n",
    "p_bvae_grid = {\n",
    "    \"beta_value\": uniform(loc=0.5, scale=9),\n",
    "    \"codings_size\": sp_randint(5, 40),\n",
    "    \"conv_layers\": [3, 2, 1],\n",
    "    \"start_filter_no\": sp_randint(16, 128),\n",
    "    \"dilations\": [[1, 2, 4, 8], [1, 2, 4], [1, 2]],\n",
    "    \"kernel_size_1\": sp_randint(2, 9),\n",
    "    \"earlystop_patience\": sp_randint(30, 50),\n",
    "}\n",
    "\n",
    "# epochs\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder to save models in\n",
    "model_save_folder = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+'_mill'\n",
    "\n",
    "# create the folder\n",
    "(folder_models / 'saved_models' / model_save_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# create dataframe to store all the results\n",
    "df_all = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Run no. 1\n",
      "Shape of Z: (None, 224)\n",
      "\n",
      " TBVAE-20210313-142759:_b=6.45_c=32_l=3_f1=28_k1=8_dil=[1, 2, 4]_p=2.65e+05_eps=1_pat=40 \n",
      "\n",
      "Train on 2831 samples, validate on 697 samples\n",
      "2831/2831 [==============================] - 13s 5ms/sample - loss: 1.8101 - rounded_accuracy: 0.7496 - val_loss: 513.6734 - val_rounded_accuracy: 0.4984\n"
     ]
    }
   ],
   "source": [
    "# setup parameters to sample\n",
    "rng = np.random.RandomState(ransdom_seed_input)\n",
    "\n",
    "# list of parameters in random search\n",
    "p_bvae = list(ParameterSampler(p_bvae_grid, n_iter=random_search_iterations,random_state=rng))\n",
    "\n",
    "\n",
    "for i, params in enumerate(p_bvae):\n",
    "    print('\\n### Run no.', i+1)\n",
    "    \n",
    "    ### TRY MODELS ###\n",
    "\n",
    "    # BETA-VAE\n",
    "    # parameters  \n",
    "    beta_value =params[\"beta_value\"]\n",
    "    codings_size =params[\"codings_size\"]\n",
    "    conv_layers =params[\"conv_layers\"]   \n",
    "    start_filter_no =params[\"start_filter_no\"] \n",
    "    kernel_size_1 = params[\"kernel_size_1\"]\n",
    "    dilations = params[\"dilations\"]\n",
    "    earlystop_patience=params[\"earlystop_patience\"]\n",
    "    \n",
    "\n",
    "    seed = 16\n",
    "    verbose = 1\n",
    "\n",
    "    # try the model and if it doesn't work, go onto the next model\n",
    "    # not always the best to use 'try' but good enough\n",
    "    try:\n",
    "\n",
    "        date_time, model_name, history, beta_vae_model, bvae_encoder = model_fit(\n",
    "            X_train_slim,\n",
    "            X_val_slim,\n",
    "            beta_value=beta_value,\n",
    "            codings_size=codings_size,\n",
    "            conv_layers=conv_layers,\n",
    "            seed=seed,\n",
    "            start_filter_no=start_filter_no,\n",
    "            kernel_size_1=kernel_size_1,\n",
    "            dilations=dilations,\n",
    "            epochs=epochs,\n",
    "            earlystop_patience=earlystop_patience,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        # save the model. How to: https://www.tensorflow.org/tutorials/keras/save_and_load\n",
    "        # save model weights and model json\n",
    "        model_save_dir_bvae = (\n",
    "            folder_models / 'saved_models' / model_save_folder / (date_time + \"_bvae\")\n",
    "        )\n",
    "        model_save_dir_encoder = (\n",
    "            folder_models / 'saved_models' / model_save_folder / (date_time + \"_encoder\")\n",
    "        )\n",
    "\n",
    "        # create the save paths\n",
    "        Path(model_save_dir_bvae).mkdir(parents=True, exist_ok=True)\n",
    "        Path(model_save_dir_encoder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # save entire bvae model\n",
    "        model_as_json = beta_vae_model.to_json()\n",
    "        with open(r\"{}/model.json\".format(str(model_save_dir_bvae)), \"w\",) as json_file:\n",
    "            json_file.write(model_as_json)\n",
    "        beta_vae_model.save_weights(str(model_save_dir_encoder) + \"/weights.h5\")\n",
    "\n",
    "        # save encoder bvae model\n",
    "        model_as_json = bvae_encoder.to_json()\n",
    "        with open(r\"{}/model.json\".format(str(model_save_dir_encoder)), \"w\",) as json_file:\n",
    "            json_file.write(model_as_json)\n",
    "        bvae_encoder.save_weights(str(model_save_dir_encoder) + \"/weights.h5\")\n",
    "\n",
    "        # get the model run history\n",
    "        results = pd.DataFrame(history.history)\n",
    "        epochs_trained = len(results)\n",
    "        results[\"epochs_trained\"] = epochs_trained\n",
    "        results = list(\n",
    "            results[results[\"val_loss\"] == results[\"val_loss\"].min()].to_numpy()\n",
    "        )  # only keep the top result, that is, the lowest val_loss\n",
    "\n",
    "        # append best result onto df_model_results dataframe\n",
    "        if i == 0:\n",
    "            cols = (\n",
    "                list(p_bvae[0].keys())\n",
    "                + list(history.history.keys())\n",
    "                + [\"epochs_trained\"]\n",
    "            )\n",
    "            results = [[p_bvae[i][k] for k in p_bvae[i]] + list(results[0])]\n",
    "\n",
    "        else:\n",
    "            # create dataframe to store best result from model training\n",
    "            cols = (\n",
    "                list(p_bvae[0].keys())\n",
    "                + list(history.history.keys())\n",
    "                + [\"epochs_trained\"]\n",
    "            )\n",
    "            results = [[p_bvae[i][k] for k in p_bvae[i]] + list(results[0])]\n",
    "\n",
    "        df = pd.DataFrame(results, columns=cols)\n",
    "\n",
    "        df[\"date_time\"] = date_time\n",
    "        df[\"model_name\"] = model_name\n",
    "\n",
    "        df_all = df_all.append(df, sort=False)\n",
    "\n",
    "        df_all.to_csv(\"results_interim_{}.csv\".format(model_save_folder))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"TRACEBACK\")\n",
    "        traceback.print_exc()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this post we've learned about the VAE; prepared the milling data; and implemented the training of the VAE. Phew! Lots of work! In the next post, we'll evaluate the trained VAEs for anomaly detection performance. I'll also explain how we use the precision-recall curve, and we'll make some pretty graphics (my favourite!). Stay tuned!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References \n",
    "\n",
    "[^kingma2013auto]: Kingma, Diederik P., and Max Welling. \"[Auto-encoding variational bayes.](https://arxiv.org/abs/1312.6114)\" arXiv preprint arXiv:1312.6114 (2013).\n",
    "\n",
    " \n",
    "\n",
    "[^higgins2016beta]: Higgins, Irina, et al. \"[beta-vae: Learning basic visual concepts with a constrained variational framework.](https://openreview.net/forum?id=Sy2fzU9gl)\" (2016).\n",
    "\n",
    "\n",
    "[^geron2019hands]: Géron, Aurélien. [Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow: Concepts, tools, and techniques to build intelligent systems.](https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646/ref=sr_1_1?dchild=1&keywords=Aur%C3%A9lien+Geron&qid=1614346360&sr=8-1) O'Reilly Media, 2019.\n",
    "\n",
    " \n",
    "\n",
    "[^bergstra2012random]: Bergstra, James, and Yoshua Bengio. \"[Random search for hyper-parameter optimization.](https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a)\" The Journal of Machine Learning Research 13.1 (2012): 281-305. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
